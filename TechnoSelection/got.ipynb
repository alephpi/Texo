{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69f35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ucaslcl/GOT-OCR2_0\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb012069",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"ucaslcl/GOT-OCR2_0\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\",\n",
    "    use_safetensors=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a850a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4322538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOTQwenForCausalLM(\n",
      "  (model): GOTQwenModel(\n",
      "    (embed_tokens): Embedding(151860, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "    (vision_tower_high): ImageEncoderViT(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLPBlock(\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (neck): Sequential(\n",
      "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): LayerNorm2d()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (3): LayerNorm2d()\n",
      "      )\n",
      "      (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (mm_projector_vary): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151860, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481a36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for param in model.parameters():\n",
    "    s += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a318f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560.52864"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s / 1e6  # in millions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f882d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for param in model.model.vision_tower_high.parameters():\n",
    "    s += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7048362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.569152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s / 1e6  # in millions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84c4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for param in model.model.layers.parameters():\n",
    "    s += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4dd78ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.404224"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s / 1e6  # in millions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4624d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for param in model.model.mm_projector_vary.parameters():\n",
    "    s += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "519e3798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0496"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s / 1e6  # in millions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a80125",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for param in model.lm_head.parameters():\n",
    "    s += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8c793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155.50464"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s / 1e6  # in millions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "308 + 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0a927f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenTokenizer(name_or_path='ucaslcl/GOT-OCR2_0', vocab_size=151860, model_max_length=8000, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f76a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input your test image\n",
    "image_file = \"xxx.jpg\"\n",
    "\n",
    "# plain texts OCR\n",
    "res = model.chat(tokenizer, image_file, ocr_type=\"ocr\")\n",
    "\n",
    "# format texts OCR:\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format')\n",
    "\n",
    "# fine-grained OCR:\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_box='')\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_box='')\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_color='')\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_color='')\n",
    "\n",
    "# multi-crop OCR:\n",
    "# res = model.chat_crop(tokenizer, image_file, ocr_type='ocr')\n",
    "# res = model.chat_crop(tokenizer, image_file, ocr_type='format')\n",
    "\n",
    "# render the formatted OCR results:\n",
    "# res = model.chat(tokenizer, image_file, ocr_type='format', render=True, save_render_file = './demo.html')\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960701f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
